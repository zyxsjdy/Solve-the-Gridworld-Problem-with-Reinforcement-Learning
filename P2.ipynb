{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"10\">Part 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from environment import Environment_2 as ENV  # class object, see the file \"environment.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Relavent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploring_starts(env, es=True):\n",
    "    \"\"\" \n",
    "    Randomely choose a start (excep from the blue, green, black grids)\n",
    "    Input:\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_row (int) - number of rows\n",
    "            env.n_col (int) - number of columns\n",
    "            env.map (2d array) - the map of the gridworld, see in environment.py\n",
    "    Output:\n",
    "        if es = True\n",
    "        (row * env.n_row + col) - return the randomely selected start state\n",
    "        if es = False\n",
    "        23 (int) - fix start state\n",
    "    \"\"\"\n",
    "    if es:\n",
    "        while 1:\n",
    "            # randomely select a position on the map\n",
    "            row = np.random.choice([i for i in range(env.n_row)])\n",
    "            col = np.random.choice([i for i in range(env.n_col)])\n",
    "            if env.map[row][col] != 'T':\n",
    "                break  # if the start is not terminal (Black), choose it\n",
    "\n",
    "        return row * env.n_row + col  # calculate the state number based on the place on the map\n",
    "    else:\n",
    "        return 23  # if don't explore starts, start from state 23\n",
    "\n",
    "def argmax_random(input_array):\n",
    "    \"\"\" \n",
    "    Randomely select the maximum value (break the ties arbitrarily)\n",
    "    Input:\n",
    "        input_array (1d array) - array needed to be found the maximum value\n",
    "    Output:\n",
    "        index for the maximum value, if multiple, randomely select one\n",
    "    \"\"\"\n",
    "    return np.random.choice(np.flatnonzero(input_array == np.max(input_array)))\n",
    "\n",
    "def print_policy(policy, env):\n",
    "    \"\"\" \n",
    "    Print the policy for visualization\n",
    "    Input:\n",
    "        policy (1d array) - a deterministic policy\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int) - number of states\n",
    "            env.action_text (1d array) - contain the unicode of the arrows for visualization\n",
    "                                             left ←    down ↓    right →   up ↑\n",
    "                                          ['\\u2190', '\\u2193', '\\u2192', '\\u2191']\n",
    "    Output:\n",
    "        policy_visual (1d array) - visualized policy, with arrows pointing to the moving direction\n",
    "    \"\"\"\n",
    "\n",
    "    policy_visual = ['' for _ in range(env.n_state)]\n",
    "\n",
    "    for s in range(env.n_state):\n",
    "        lenth = len(policy[s])\n",
    "        if lenth == 4:\n",
    "            policy_visual[s] += 'o'  # 'o' means 4 directions are all available\n",
    "        else:\n",
    "            for a in range(lenth):\n",
    "                policy_visual[s] += env.action_text[policy[s][a]]\n",
    "\n",
    "    return policy_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Part 2.1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Monte Carlo method with exploring starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCM_exploring_starts(max_ep, gamma, env):\n",
    "    \"\"\" \n",
    "    Monte Carlo method with exploring starts\n",
    "    Input:\n",
    "        max_ep (int) - maximum number of episodes\n",
    "        gamma (float) - reward discount factor\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int) - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (4) array, for state s and action a,\n",
    "                                    there are n possibilities, each row is composed of (p, s_, r, t)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "                                    t  - terminal information, a bool value, True/False\n",
    "    Output:\n",
    "        Q_all (2d array) - Estimated Q function, for each state and each action\n",
    "        Q_opt (1d array) - Estimated optimal Q function\n",
    "        policy_opt (2d array) - Calculated optimum policy, the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    policy_opt = [[0] for _ in range(env.n_state)]  # initialize a deterministic policy\n",
    "    Return = [[[] for _ in range(env.n_action)] for _ in range(env.n_state)]  # Return\n",
    "    Q_all = [[0 for _ in range(env.n_action)] for _ in range(env.n_state)]  # Q function\n",
    "    Q_opt = [0 for _ in range(env.n_state)]\n",
    "\n",
    "    for ep in range(max_ep):\n",
    "        s = exploring_starts(env, es=True)  # randomely chose the start point\n",
    "        Traces = []  # initialize the moving trajectory\n",
    "        \n",
    "        # Generate an episode\n",
    "        while 1:\n",
    "            a = np.random.randint(4) # action under the equiprobable policy\n",
    "\n",
    "            # determine the next state, reward, terminal\n",
    "            temp = env.model[s][a]\n",
    "            # if there are multiple choice, e.g., at green, randomely select one based on p\n",
    "            p_next = np.random.choice([i for i in range(len(temp))], p = [temp[i][0] for i in range(len(temp))])  \n",
    "            _, s_, r, t = temp[p_next]\n",
    "\n",
    "            Traces.append([s, a, r])  # generate the moving trajectory\n",
    "\n",
    "            s = s_  # update state\n",
    "\n",
    "            if t:  # if terminal, break\n",
    "                break\n",
    "        \n",
    "        # Loop for each step of the episode\n",
    "        G = 0  # initialize G\n",
    "        for index, trace in enumerate(Traces[::-1]):  # Start from the end of the trajectory\n",
    "            G = gamma * G + trace[2]  # update G\n",
    "            \n",
    "            # Unless the pair (s,a) appears, i.e., first visit check\n",
    "            if not (trace[0], trace[1]) in [(np.array(Traces[::-1])[i,0], np.array(Traces[::-1])[i,1]) for i in range(index+1, len(Traces[::-1]))]:\n",
    "                Return[trace[0]][trace[1]].append(G)\n",
    "                Q_all[trace[0]][trace[1]] = np.mean(Return[trace[0]][trace[1]])\n",
    "\n",
    "    for s in range(env.n_state):  # sweep all the states in the state space\n",
    "        policy_opt[s] = (np.unique(np.argwhere(Q_all[s] == np.max(Q_all[s])))).tolist()  # find the optimal policy under the Q function\n",
    "\n",
    "    Q_opt = np.max(Q_all, axis=1)  # find the optimal Q function\n",
    "\n",
    "    return Q_all, Q_opt, policy_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Monte Carlo method with ϵ-soft approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCM_epsilon_soft(max_ep, gamma, epsilon, env):\n",
    "    \"\"\" \n",
    "    Monte Carlo method with ϵ-soft approach\n",
    "    Input:\n",
    "        max_ep (int) - maximum number of episodes\n",
    "        gamma (float) - reward discount factor\n",
    "        epsilon - Algorithm parameter: small ϵ > 0\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int) - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (4) array, for state s and action a,\n",
    "                                    there are n possibilities, each row is composed of (p, s_, r, t)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "                                    t  - terminal information, a bool value, True/False\n",
    "    Output:\n",
    "        Q_all (2d array) - Estimated Q function, for each state and each action\n",
    "        Q_opt (1d array) - Estimated optimal Q function\n",
    "        policy_opt (2d array) - Calculated optimum policy, the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    policy = np.ones([env.n_state, env.n_action]) / env.n_action  # initialize policy, the sum of each row is 1\n",
    "    policy_opt = [[0] for _ in range(env.n_state)]  # initialize a deterministic policy\n",
    "    Return = [[[] for _ in range(env.n_action)] for _ in range(env.n_state)]  # Return\n",
    "    Q_all = [[0 for _ in range(env.n_action)] for _ in range(env.n_state)]  # Q function\n",
    "    Q_opt = [0 for _ in range(env.n_state)]\n",
    "\n",
    "    for ep in range(max_ep):\n",
    "        s = exploring_starts(env, es=False)  # fix start point\n",
    "        Traces = []  # initialize the moving trajectory\n",
    "        \n",
    "        # Generate an episode\n",
    "        while 1:\n",
    "            a = np.random.choice(np.arange(env.n_action),p=policy[s])  # choose the action under the current policy\n",
    "\n",
    "            # determine the next state, reward, terminal\n",
    "            temp = env.model[s][a]\n",
    "            p_next = np.random.choice([i for i in range(len(temp))], p = [temp[i][0] for i in range(len(temp))])  # if there are multiple choice, e.g., at green\n",
    "            _, s_, r, t = temp[p_next]\n",
    "\n",
    "            Traces.append([s, a, r])  # generate the moving trajectory\n",
    "\n",
    "            s = s_  # update state\n",
    "\n",
    "            if t:  # if terminal, break\n",
    "                break\n",
    "        \n",
    "        # Loop for each step of the episode\n",
    "        G = 0  # initialize G\n",
    "        for index, trace in enumerate(Traces[::-1]):  # Start from the end of the trajectory\n",
    "            G = gamma * G + trace[2]  # update G\n",
    "            \n",
    "            # Unless the pair (s,a) appears, i.e., first visit check\n",
    "            if not (trace[0], trace[1]) in [(np.array(Traces[::-1])[i,0], np.array(Traces[::-1])[i,1]) for i in range(index+1, len(Traces[::-1]))]:\n",
    "                Return[trace[0]][trace[1]].append(G)\n",
    "                Q_all[trace[0]][trace[1]] = np.mean(Return[trace[0]][trace[1]])\n",
    "\n",
    "                A_star = argmax_random(Q_all[trace[0]])  # find A*, with ties broken arbitratily\n",
    "\n",
    "                for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "                    # determine the policy for the current state based on Q\n",
    "                    if a == A_star:\n",
    "                        policy[trace[0], a] = 1 - epsilon + epsilon/env.n_action\n",
    "                    else:\n",
    "                        policy[trace[0], a] = epsilon/env.n_action\n",
    "\n",
    "    for s in range(env.n_state):  # sweep all the states in the state space\n",
    "        policy_opt[s] = (np.unique(np.argwhere(policy[s] == np.max(policy[s])))).tolist()  # find the optimal policy under the Q function\n",
    "\n",
    "    Q_opt = np.max(Q_all, axis=1)  # find the optimal Q function\n",
    "\n",
    "    return Q_all, Q_opt, policy_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo method with exploring starts:\n",
      "[['→' '→' '←' '→' '←']\n",
      " ['↑' '↑' '↑' '→' '↑']\n",
      " ['↑' '↑' '↑' '→' 'o']\n",
      " ['↓' '↑' '↑' '↑' '↑']\n",
      " ['o' '←' '←' '↑' '↑']] \n",
      "\n",
      "Monte Carlo method with ϵ-soft approach:\n",
      "[['→' '↑' '←' '←' '→']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' 'o']\n",
      " ['↓' '↑' '↑' '↑' '↑']\n",
      " ['o' '←' '←' '↑' '↑']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment, set up parameters\n",
    "max_ep = 5000\n",
    "gam = 0.95\n",
    "eps = 0.85  # epsilon, Algorithm parameter: small ϵ > 0\n",
    "Env = ENV()\n",
    "\n",
    "# Find the optimal policy\n",
    "Q_1, Q_opt_1, pol_opt_1 = MCM_exploring_starts(max_ep, gam, Env)\n",
    "Q_2, Q_opt_2, pol_opt_2 = MCM_epsilon_soft(max_ep, gam, eps, Env)\n",
    "\n",
    "# Print the results\n",
    "print('Monte Carlo method with exploring starts:')\n",
    "# print(np.array(Q_1), '\\n')\n",
    "print((np.array(print_policy(pol_opt_1, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "print('Monte Carlo method with ϵ-soft approach:')\n",
    "# print(np.array(Q_2), '\\n')\n",
    "print((np.array(print_policy(pol_opt_2, Env))).reshape([5, -1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo method with ϵ-soft approach: \n",
      "\n",
      "ϵ = 0.9\n",
      "[['→' '↑' '←' '←' '↑']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '→' 'o']\n",
      " ['↓' '↑' '↑' '↑' '↑']\n",
      " ['o' '←' '←' '↑' '↑']] \n",
      "\n",
      "ϵ = 0.7\n",
      "[['→' '←' '←' '←' '→']\n",
      " ['↑' '↑' '↑' '←' '↑']\n",
      " ['↑' '↑' '↑' '↑' 'o']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['o' '↑' '↑' '↑' '↑']] \n",
      "\n",
      "ϵ = 0.5\n",
      "[['→' '←' '←' '←' '↑']\n",
      " ['↑' '↑' '↑' '←' '↑']\n",
      " ['→' '↑' '↑' '↑' 'o']\n",
      " ['→' '↑' '↑' '←' '←']\n",
      " ['o' '↑' '↑' '↑' '←']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Monte Carlo method with ϵ-soft approach: \\n')\n",
    "max_ep = 5000\n",
    "eps = 0.90\n",
    "Q_2_1, Q_opt_2_1, pol_opt_2_1 = MCM_epsilon_soft(max_ep, gam, eps, Env)\n",
    "print('ϵ =', eps)\n",
    "print((np.array(print_policy(pol_opt_2_1, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "max_ep = 1000\n",
    "eps = 0.70\n",
    "Q_2_2, Q_opt_2_2, pol_opt_2_2 = MCM_epsilon_soft(max_ep, gam, eps, Env)\n",
    "print('ϵ =', eps)\n",
    "print((np.array(print_policy(pol_opt_2_2, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "# !!!!!!!!!!!!! Take a long time to run (around 15 min) !!!!!!!!!!!!!!!!\n",
    "max_ep = 500\n",
    "eps = 0.50\n",
    "Q_2_3, Q_opt_2_3, pol_opt_2_3 = MCM_epsilon_soft(max_ep, gam, eps, Env)\n",
    "print('ϵ =', eps)\n",
    "print((np.array(print_policy(pol_opt_2_3, Env))).reshape([5, -1]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Part 2.2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a behaviour policy with equiprobable moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCM_behaviour_policy(max_ep, gamma, env):\n",
    "    \"\"\" \n",
    "    Monte Carlo method with behaviour policy with equiprobable moves\n",
    "    Input:\n",
    "        max_ep (int) - maximum number of episodes\n",
    "        gamma (float) - reward discount factor\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int) - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (4) array, for state s and action a,\n",
    "                                    there are n possibilities, each row is composed of (p, s_, r, t)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "                                    t  - terminal information, a bool value, True/False\n",
    "    Output:\n",
    "        Q_all (2d array) - Estimated Q function, for each state and each action\n",
    "        Q_opt (1d array) - Estimated optimal Q function\n",
    "        policy_opt (2d array) - Calculated optimum policy, the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    policy_opt = [[0] for _ in range(env.n_state)]  # initialize a deterministic policy\n",
    "    # Q_all = [[(random.random()) for _ in range(env.n_action)] for _ in range(env.n_state)]  # Q function\n",
    "    Q_all = [[-0.21 for _ in range(env.n_action)] for _ in range(env.n_state)]  # Q function\n",
    "    \n",
    "    Q_opt = [0.0 for _ in range(env.n_state)]\n",
    "    C = [[0.0 for _ in range(env.n_action)] for _ in range(env.n_state)]  # cumulative sum\n",
    "    \n",
    "    pi = np.argmax(Q_all, axis=1)  # initialize the optimal policy with ties broken consistently (np.argmax() always selects the first largest term)\n",
    "    b = np.ones([env.n_state, env.n_action]) / env.n_action  # equiprobable behaviour policy\n",
    "\n",
    "    for ep in range(max_ep):\n",
    "        # s = exploring_starts(env, es=False)  # fix start point\n",
    "        s = exploring_starts(env, es=True)  # exploring starts\n",
    "\n",
    "        Traces = []  # initialize the moving trajectory\n",
    "        \n",
    "        # Generate an episode\n",
    "        while 1:\n",
    "            a = np.random.choice(np.arange(env.n_action),p=b[s])  # choose the action under the equiprobable behaviour policy\n",
    "            \n",
    "            # determine the next state, reward, terminal\n",
    "            temp = env.model[s][a]\n",
    "            p_next = np.random.choice([i for i in range(len(temp))], p = [temp[j][0] for j in range(len(temp))])  # if there are multiple choice, e.g., at green\n",
    "            _, s_, r, t = temp[p_next]\n",
    "\n",
    "            Traces.append([s, a, r])  # generate the moving trajectory\n",
    "\n",
    "            s = s_  # update state\n",
    "\n",
    "            if t:  # if terminal, break\n",
    "                break\n",
    "        \n",
    "        # Loop for each step of the episode\n",
    "        G = 0  # initialize G\n",
    "        W = 1  # initialize weight\n",
    "        for index, trace in enumerate(Traces[::-1]):  # Start from the end of the trajectory\n",
    "            G = gamma * G + trace[2]  # update G\n",
    "            C[trace[0]][trace[1]] += W  # update C\n",
    "            Q_all[trace[0]][trace[1]] += (W / C[trace[0]][trace[1]]) * (G - Q_all[trace[0]][trace[1]])  # update Q\n",
    "\n",
    "            pi[trace[0]] = np.argmax(Q_all[trace[0]])  # update policy\n",
    "\n",
    "            if trace[1] != pi[trace[0]]: # check whether the policy takes the same action as the behaviour policy\n",
    "                break\n",
    "\n",
    "            W = W / b[trace[0]][trace[1]]  # update W\n",
    "\n",
    "\n",
    "    for s in range(env.n_state):  # sweep all the states in the state space\n",
    "        policy_opt[s] = (np.unique(np.argwhere(Q_all[s] == np.max(Q_all[s])))).tolist()  # find the optimal policy under the Q function\n",
    "\n",
    "    Q_opt = np.max(Q_all, axis=1)  # find the optimal Q function\n",
    "\n",
    "    return Q_all, Q_opt, policy_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo method with behaviour policy with equiprobable moves:\n",
      "[['→' 'o' '←' '→' 'o']\n",
      " ['→↑' '↑' '←↑' '↑' '↑']\n",
      " ['↓' '↑' '←↑' '←' 'o']\n",
      " ['↓' '→' '↑' '→↑' '↑']\n",
      " ['o' '→' '↑' '↓' '↑']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment, set up parameters\n",
    "max_ep = 200000\n",
    "gam = 0.95\n",
    "Env = ENV()\n",
    "\n",
    "# Find the optimal policy\n",
    "Q_3, Q_opt_3, pol_opt_3 = MCM_behaviour_policy(max_ep, gam, Env)\n",
    "\n",
    "# Print the results\n",
    "print('Monte Carlo method with behaviour policy with equiprobable moves:')\n",
    "# print(np.array(Q_3), '\\n')\n",
    "print((np.array(print_policy(pol_opt_3, Env))).reshape([5, -1]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Part 2.3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Policy iteration (permute the locations of the green and blue squares with probability 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCM_policy_iteration(max_ep, gamma, env, permute=False):\n",
    "    \"\"\" \n",
    "    Monte Carlo method with policy iteration (permute the locations of the green and blue squares with probability 0.1)\n",
    "    Input:\n",
    "        max_ep (int) - maximum number of episodes\n",
    "        gamma (float) - reward discount factor\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int) - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (4) array, for state s and action a,\n",
    "                                    there are n possibilities, each row is composed of (p, s_, r, t)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "                                    t  - terminal information, a bool value, True/False\n",
    "        permute (bool) - whether the green and blue squares will permute, default as False\n",
    "    Output:\n",
    "        Q_all (2d array) - Estimated Q function, for each state and each action\n",
    "        Q_opt (1d array) - Estimated optimal Q function\n",
    "        policy_opt (2d array) - Calculated optimum policy, the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    policy = np.ones([env.n_state, env.n_action]) / env.n_action  # initialize policy, the sum of each row is 1\n",
    "    epsilon = 0.85\n",
    "\n",
    "    policy_opt = [[0] for _ in range(env.n_state)]  # initialize a deterministic policy\n",
    "\n",
    "    Q_all = [[0.0 for _ in range(env.n_action)] for _ in range(env.n_state)]  # Q function\n",
    "    Q_opt = [0.0 for _ in range(env.n_state)]\n",
    "    C = [[0.0 for _ in range(env.n_action)] for _ in range(env.n_state)]  # cumulative sum\n",
    "    \n",
    "    b = np.ones([env.n_state, env.n_action]) / env.n_action  # equiprobable behaviour policy\n",
    "\n",
    "    for ep in range(max_ep):\n",
    "        s = exploring_starts(env, es=True)  # exploring starts\n",
    "        Traces = []  # initialize the moving trajectory\n",
    "\n",
    "        # Generate an episode\n",
    "        while 1:\n",
    "            a = np.random.choice(np.arange(env.n_action),p=b[s])  # choose the action under the equiprobable behaviour policy\n",
    "\n",
    "            # determine the next state, reward, terminal\n",
    "            temp = env.model[s][a]\n",
    "            p_next = np.random.choice([i for i in range(len(temp))], p = [temp[i][0] for i in range(len(temp))])  # if there are multiple choice, e.g., at green\n",
    "            _, s_, r, t = temp[p_next]\n",
    "\n",
    "            Traces.append([s, a, r])  # generate the moving trajectory\n",
    "\n",
    "            s = s_  # update state\n",
    "\n",
    "            # permute with probability 0.1 if permute = True\n",
    "            if permute and (np.random.randint(10) == 0):\n",
    "                env.permute_G_B()\n",
    "\n",
    "            if t:  # if terminal, break\n",
    "                break\n",
    "        \n",
    "        # Loop for each step of the episode\n",
    "        G = 0  # initialize G\n",
    "        W = 1  # initialize weight\n",
    "        for index, trace in enumerate(Traces[::-1]):  # Start from the end of the trajectory\n",
    "            G = gamma * G + trace[2]  # update G\n",
    "            C[trace[0]][trace[1]] += W  # update C\n",
    "            Q_all[trace[0]][trace[1]] += W / C[trace[0]][trace[1]] * (G - Q_all[trace[0]][trace[1]])  # update Q\n",
    "\n",
    "            # use ϵ-soft approach to update the policy\n",
    "            A_star = argmax_random(Q_all[trace[0]])  # find A*, with ties broken arbitratily\n",
    "\n",
    "            for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "                # determine the policy for the current state based on Q\n",
    "                if a == A_star:\n",
    "                    policy[trace[0], a] = 1 - epsilon + epsilon/env.n_action\n",
    "                else:\n",
    "                    policy[trace[0], a] = epsilon/env.n_action\n",
    "\n",
    "            W = W * (policy[trace[0], trace[1]] / b[trace[0]][trace[1]])  # update W\n",
    "\n",
    "    for s in range(env.n_state):  # sweep all the states in the state space\n",
    "        policy_opt[s] = (np.unique(np.argwhere(policy[s] == np.max(policy[s])))).tolist()  # find the optimal policy under the Q function\n",
    "\n",
    "    Q_opt = np.max(Q_all, axis=1)  # find the optimal Q function\n",
    "\n",
    "    return Q_all, Q_opt, policy_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo method with policy iteration (permute the blue and green squares):\n",
      "[['→' '↑' '←' '→' '←']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' 'o']\n",
      " ['↓' '↑' '↑' '↑' '↑']\n",
      " ['o' '←' '←' '↑' '↑']] \n",
      "\n",
      "Monte Carlo method with policy iteration:\n",
      "[['→' '←' '←' '←' '→']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' 'o']\n",
      " ['↓' '↑' '↑' '↑' '↑']\n",
      " ['o' '←' '←' '↑' '↑']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment, set up parameters\n",
    "max_ep = 50000\n",
    "gam = 0.95\n",
    "Env = ENV()\n",
    "\n",
    "# Find the optimal policy\n",
    "Q_4, Q_opt_4, pol_opt_4 = MCM_policy_iteration(max_ep, gam, Env, permute=True)\n",
    "Q_5, Q_opt_5, pol_opt_5 = MCM_policy_iteration(max_ep, gam, Env)\n",
    "\n",
    "# Print the results\n",
    "print('Monte Carlo method with policy iteration (permute the blue and green squares):')\n",
    "# print(np.array(Q_4), '\\n')\n",
    "print((np.array(print_policy(pol_opt_4, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "print('Monte Carlo method with policy iteration:')\n",
    "# print(np.array(Q_5), '\\n')\n",
    "print((np.array(print_policy(pol_opt_5, Env))).reshape([5, -1]), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
