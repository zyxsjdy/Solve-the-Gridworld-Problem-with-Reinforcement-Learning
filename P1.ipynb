{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"10\">Part 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from environment import Environment_1 as ENV  # class object, see the file \"environment.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Part 1.1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Solving the system of Bellman equations explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_equa(gamma, policy, env):\n",
    "    \"\"\" \n",
    "    Bellman equations\n",
    "    Input:\n",
    "        gamma (float) - reward discount factor\n",
    "        policy (2d array) - policy, the sum of each row is 1\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int)    - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (3) array, for state s and action a,\n",
    "                                   there are n possibilities, each row is composed of (p, s_, r)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n possibilities -- n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "    Output:\n",
    "        VF (1d array) - Calculated value function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the system of linear equations\n",
    "    # Calculate the value function by (A * VF = b)\n",
    "    A = np.zeros([env.n_state, env.n_state])\n",
    "    b = np.zeros(env.n_state)\n",
    "\n",
    "    for s in range(env.n_state):  # sweep all the states in the state space\n",
    "        A[s,s] = 1  # VF[s] = sum(policy[s,a] * p * (r + gamma * VF[s_])), hence the coefficient for VF[s] is 1\n",
    "        for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "            for p, s_, r in env.model[s][a]:  # sweep all the possibilities in model\n",
    "                A[s,s_] -= policy[s,a] * p * gamma  # put the coefficients of VF[s_] to the left of the equation, hence minus\n",
    "                b[s] += policy[s,a] * p * r  # keep the constants to the right of the equation\n",
    "\n",
    "    # VF = A' * b, value function equals the procuct of inverse A and b\n",
    "    if np.linalg.det(A) == 0: raise ValueError(\"A is not invertible !!!!!!!!!!\")\n",
    "    A_ = np.linalg.inv(A)\n",
    "    VF = np.dot(A_, b)\n",
    "    return VF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Iter_policy_eval(theta, gamma, policy, env):\n",
    "    \"\"\" \n",
    "    Iterative Policy Evaluation\n",
    "    Input:\n",
    "        theta (float) - a small threshold > 0 determining accuracy of estimation\n",
    "        gamma (float) - reward discount factor\n",
    "        policy (2d array) - policy, the sum of each row is 1\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int)    - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (3) array, for state s and action a,\n",
    "                                   there are n possibilities, each row is composed of (p, s_, r)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n possibilities -- n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "    Output:\n",
    "        VF (1d array) - Estimated value function\n",
    "    \"\"\"\n",
    "    \n",
    "    VF = np.zeros(env.n_state)  # initialize value function\n",
    "\n",
    "    while 1:  # loop untill converge\n",
    "        delta = 0  # initialize delta\n",
    "\n",
    "        for s in range(env.n_state):  # sweep all the states in the state space\n",
    "            v = VF[s]  # store the old value function\n",
    "\n",
    "            # estimate the new value function\n",
    "            temp = 0\n",
    "            for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "                for p, s_, r in env.model[s][a]:  # sweep all the possibilities in model\n",
    "                    temp += policy[s,a] * p * (r + gamma * VF[s_])\n",
    "            VF[s] = temp  # new value function\n",
    "            \n",
    "            delta = max(delta, abs(v - VF[s]))  # update the gap between the old and new value functions\n",
    "\n",
    "        if delta < theta:  # stop when the gap is sufficiently small, i.e., the value function converges\n",
    "            return VF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value_iter(theta, gamma, env):\n",
    "    \"\"\" \n",
    "    Value iteration\n",
    "    Input:\n",
    "        theta (float) - a small threshold > 0 determining accuracy of estimation\n",
    "        gamma (float) - reward discount factor\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int)    - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (3) array, for state s and action a,\n",
    "                                   there are n possibilities, each row is composed of (p, s_, r)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n possibilities -- n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "    Output:\n",
    "        VF (1d array) - Estimated value function\n",
    "    \"\"\"\n",
    "\n",
    "    VF = np.zeros(env.n_state)  # initialize value function\n",
    "\n",
    "    while 1:  # loop untill converge\n",
    "        delta = 0  # initialize delta\n",
    "\n",
    "        for s in range(env.n_state):  # sweep all the states in the state space\n",
    "            v = VF[s]  # store the old value function\n",
    "\n",
    "            # estimate the new value function\n",
    "            temp1 = []\n",
    "            for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "                temp2 = 0\n",
    "                for p, s_, r in env.model[s][a]:  # sweep all the possibilities in model\n",
    "                    temp2 += p * (r + gamma * VF[s_])\n",
    "                temp1.append(temp2)\n",
    "            VF[s] = max(temp1)  # new value function\n",
    "            \n",
    "            delta = max(delta, abs(v - VF[s]))  # update the gap between the old and new value functions\n",
    "\n",
    "        if delta < theta:  # stop when the gap is sufficiently small, i.e., the value function converges\n",
    "            return VF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellman equation:\n",
      "[[ 2.17100208  4.7336156   2.07028049  1.26529444  1.77912239]\n",
      " [ 1.1180732   1.7821227   1.17409573  0.739174    0.56246548]\n",
      " [ 0.16279444  0.47788999  0.35198379  0.11045592 -0.18617038]\n",
      " [-0.54699155 -0.28473257 -0.28040463 -0.43990985 -0.7443105 ]\n",
      " [-1.10787684 -0.84936779 -0.80799244 -0.93799278 -1.23723244]] \n",
      "\n",
      "Iterative Policy Evaluation:\n",
      "[[ 2.17100208  4.7336156   2.07028049  1.26529444  1.77912239]\n",
      " [ 1.1180732   1.7821227   1.17409573  0.739174    0.56246548]\n",
      " [ 0.16279445  0.47788999  0.35198379  0.11045592 -0.18617037]\n",
      " [-0.54699155 -0.28473257 -0.28040463 -0.43990985 -0.7443105 ]\n",
      " [-1.10787684 -0.84936779 -0.80799244 -0.93799278 -1.23723244]] \n",
      "\n",
      "Value iteration:\n",
      "[[20.99734632 22.10246981 20.99734632 19.94747901 18.38284993]\n",
      " [19.94747901 20.99734632 19.94747901 18.95010506 18.0025998 ]\n",
      " [18.95010506 19.94747901 18.95010506 18.0025998  17.10246981]\n",
      " [18.0025998  18.95010506 18.0025998  17.10246981 16.24734632]\n",
      " [17.10246981 18.0025998  17.10246981 16.24734632 15.43497901]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment, set up parameters\n",
    "the = 1e-10\n",
    "gam = 0.95\n",
    "Env = ENV()\n",
    "pol = np.ones([Env.n_state, Env.n_action]) / Env.n_action  # initialize policy, the sum of each row is 1\n",
    "\n",
    "# Calculate the value function via the three approaches\n",
    "V_pi_1 = Bellman_equa(gam, pol, Env)\n",
    "V_pi_2 = Iter_policy_eval(the, gam, pol, Env)\n",
    "V_pi_3 = Value_iter(the, gam, Env)\n",
    "\n",
    "# Print the results\n",
    "print('Bellman equation:')\n",
    "print(V_pi_1.reshape([5, -1]), '\\n')\n",
    "print('Iterative Policy Evaluation:')\n",
    "print(V_pi_2.reshape([5, -1]), '\\n')\n",
    "print('Value iteration:')\n",
    "print(V_pi_3.reshape([5, -1]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Part 1.2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Relavent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_policy(gamma, V, env):\n",
    "    \"\"\" \n",
    "    Find the optimal policy under the current value function\n",
    "    Input:\n",
    "        gamma (float) - reward discount factor\n",
    "        V (1d array) - Input value function\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int)    - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (3) array, for state s and action a,\n",
    "                                   there are n possibilities, each row is composed of (p, s_, r)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n possibilities -- n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "    Output:\n",
    "        pol (2d array) - Calculated optimum policy under the input value function\n",
    "                         the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "\n",
    "    pol = [[0] for _ in range(env.n_state)]  # initialize a deterministic policy\n",
    "\n",
    "    for s in range(env.n_state):  # sweep all the states in the state space\n",
    "        temp1 = []\n",
    "        for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "            temp2 = 0\n",
    "            for p, s_, r in env.model[s][a]:  # sweep all the possibilities in model\n",
    "                temp2 += p * (r + gamma * V[s_])\n",
    "            temp1.append(temp2)\n",
    "                \n",
    "        pol[s] = (np.unique(np.argwhere(temp1 == np.max(temp1)))).tolist()  # find the optimal policy under the current value function\n",
    "        \n",
    "    return pol\n",
    "\n",
    "def print_policy(policy, env):\n",
    "    \"\"\" \n",
    "    Print the policy for visualization\n",
    "    Input:\n",
    "        policy (1d array) - a deterministic policy\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int) - number of states\n",
    "            env.action_text (1d array) - contain the unicode of the arrows for visualization\n",
    "                                             left ←    down ↓    right →   up ↑\n",
    "                                          ['\\u2190', '\\u2193', '\\u2192', '\\u2191']\n",
    "    Output:\n",
    "        policy_visual (1d array) - visualized policy, with arrows pointing to the moving direction\n",
    "    \"\"\"\n",
    "    policy_visual = ['' for _ in range(env.n_state)]\n",
    "\n",
    "    for s in range(env.n_state):\n",
    "        lenth = len(policy[s])\n",
    "        if lenth == 4:\n",
    "            policy_visual[s] += 'o'  # 'o' means 4 directions are all available\n",
    "        else:\n",
    "            for a in range(lenth):\n",
    "                policy_visual[s] += env.action_text[policy[s][a]]\n",
    "\n",
    "    return policy_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explicitly solving the Bellman optimality equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_opti_equa(gamma, env):\n",
    "    \"\"\" \n",
    "    Bellman optimality equation\n",
    "    Input:\n",
    "        gamma (float) - reward discount factor\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int)    - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (3) array, for state s and action a,\n",
    "                                   there are n possibilities, each row is composed of (p, s_, r)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n possibilities -- n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "    Output:\n",
    "        V_opt (1d array) - Calculated value function\n",
    "        policy_opt (2d array) - Calculated optimum policy, the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "    \n",
    "    ################# Bellman optimality equation #################\n",
    "    VF = cp.Variable(env.n_state)  # initialize the optimization problem for cvx\n",
    "\n",
    "    con = []  # initialize the constraints\n",
    "    for s in range(env.n_state):  # sweep all the states in the state space\n",
    "        bellman = []\n",
    "        for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "            temp = 0\n",
    "            for p, s_, r in env.model[s][a]:  # sweep all the possibilities in model\n",
    "                temp += p * (r + gamma * VF[s_])\n",
    "            bellman.append(temp)  # right hand side of the Bellman optimality equation for each action\n",
    "\n",
    "        # for state s, the constraints is to find the maximum of the right hand side of the Bellman optimality equation\n",
    "        # '>=' cannot be set as '==', which will make the problem does not follow DCP rules.\n",
    "        con.append(VF[s] >= cp.max(cp.hstack(bellman)))\n",
    "\n",
    "    # The objective is to minimize the value function\n",
    "    # if do not set this, the result will be very large (more than 100), since this problem has multiple solutions\n",
    "    obj = cp.Minimize(cp.sum(VF)/env.n_state)  # divided by the number of states, weight each value in the value function equally\n",
    "\n",
    "    # Solve this convex problem, find the optimal value function\n",
    "    problem = cp.Problem(obj, con)\n",
    "    problem.solve()\n",
    "    V_opt = VF.value\n",
    "\n",
    "    ################# Get Optimal Policy #################\n",
    "    policy_opt = find_policy(gamma, V_opt, env)\n",
    "\n",
    "    return V_opt, policy_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Policy iteration with iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy_iter_with_Iter_policy_eval(theta, gamma, env):\n",
    "    \"\"\" \n",
    "    Policy iteration with iterative policy evaluation\n",
    "    Input:\n",
    "        theta (float) - a small threshold > 0 determining accuracy of estimation\n",
    "        gamma (float) - reward discount factor\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int)    - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (3) array, for state s and action a,\n",
    "                                   there are n possibilities, each row is composed of (p, s_, r)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n possibilities -- n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "    Output:\n",
    "        VF (1d array) - Estimated value function\n",
    "        policy (2d array) - Calculated optimum policy, the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "    \n",
    "    VF = np.zeros(env.n_state)  # initialize value function\n",
    "    policy = [[0] for _ in range(env.n_state)]  # initialize a deterministic policy\n",
    "\n",
    "    while 1:  # loop untill policy converges\n",
    "        ################# Policy Evaluation #################\n",
    "        while 1:  # loop untill value function converges\n",
    "            delta = 0  # initialize delta\n",
    "\n",
    "            for s in range(env.n_state):  # sweep all the states in the state space\n",
    "                v = VF[s]  # store the old value function\n",
    "\n",
    "                # estimate the new value function\n",
    "                temp = 0\n",
    "                for p, s_, r in env.model[s][policy[s][0]]:  # sweep all the possibilities in model\n",
    "                    temp += p * (r + gamma * VF[s_])\n",
    "                VF[s] = temp  # new value function\n",
    "            \n",
    "                delta = max(delta, abs(v - VF[s]))  # update the gap between the old and new value functions\n",
    "\n",
    "            if delta < theta:  # stop when the gap is sufficiently small, i.e., the value function converges\n",
    "                break\n",
    "        \n",
    "        ################# Policy Improvement #################\n",
    "        policy_old = policy.copy()  # store the old policy\n",
    "        policy = find_policy(gamma, VF, env)  # find the new policy\n",
    "\n",
    "        if policy_old == policy:  # stop when the old policy equals the new policy, i.e., the policy converges\n",
    "            return VF, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Policy improvement with value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy_improvement_with_Value_iter(theta, gamma, env):\n",
    "    \"\"\" \n",
    "    Policy improvement with Value iteration\n",
    "    Input:\n",
    "        theta (float) - a small threshold > 0 determining accuracy of estimation\n",
    "        gamma (float) - reward discount factor\n",
    "        env (class) - gridworld environment with the following parameters:\n",
    "            env.n_state (int)    - number of states\n",
    "            env.n_action (int)   - number of actions\n",
    "            env.model (4d array) - (n_state) by (n_action) by (n) by (3) array, for state s and action a,\n",
    "                                   there are n possibilities, each row is composed of (p, s_, r)\n",
    "                                    p  - transition probability from (s,a) to (s_)   ### sum of the n possibilities -- n p equals 1\n",
    "                                    s_ - next state\n",
    "                                    r  - reward of the transition from (s,a) to (s_)\n",
    "    Output:\n",
    "        VF (1d array) - Estimated value function\n",
    "        policy (2d array) - Calculated optimum policy, the first dimension is for states, the second dimension is for possible multiple choices\n",
    "    \"\"\"\n",
    "\n",
    "    VF = np.zeros(env.n_state)  # initialize value function\n",
    "    policy = [[0] for _ in range(env.n_state)]  # initialize a deterministic policy\n",
    "\n",
    "    while 1:  # loop untill policy converges\n",
    "        ################# Value iteration #################\n",
    "        while 1:  # loop untill value function converges\n",
    "            delta = 0  # initialize delta\n",
    "\n",
    "            for s in range(env.n_state):  # sweep all the states in the state space\n",
    "                v = VF[s]  # store the old value function\n",
    "\n",
    "                # estimate the new value function\n",
    "                temp1 = []\n",
    "                for a in range(env.n_action):  # sweep all the actions in the action space\n",
    "                    temp2 = 0\n",
    "                    for p, s_, r in env.model[s][a]:  # sweep all the possibilities in model\n",
    "                        temp2 += p * (r + gamma * VF[s_])\n",
    "                    temp1.append(temp2)\n",
    "                VF[s] = max(temp1)  # new value function\n",
    "            \n",
    "                delta = max(delta, abs(v - VF[s]))  # update the gap between the old and new value functions\n",
    "\n",
    "            if delta < theta:  # stop when the gap is sufficiently small, i.e., the value function converges\n",
    "                break\n",
    "        \n",
    "        ################# Policy Improvement #################\n",
    "        policy_old = policy.copy()  # store the old policy\n",
    "        policy = find_policy(gamma, VF, env)  # find the new policy\n",
    "\n",
    "        if policy_old == policy:  # stop when the old policy equals the new policy, i.e., the policy converges\n",
    "            return VF, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellman optimality equation:\n",
      "[[20.99734632 22.1024698  20.99734631 19.947479   18.38284995]\n",
      " [19.947479   20.99734631 19.947479   18.95010505 18.0025998 ]\n",
      " [18.95010506 19.947479   18.95010505 18.0025998  17.10246982]\n",
      " [18.00259981 18.95010505 18.00259979 17.10246981 16.24734633]\n",
      " [17.10246983 18.0025998  17.10246981 16.24734633 15.43497902]] \n",
      "\n",
      "[['→' 'o' '←' '←' 'o']\n",
      " ['↑' '↑' '↑' '↑' '←']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '←' '↑' '↑']] \n",
      "\n",
      "Policy iteration with iterative policy evaluation:\n",
      "[[20.99734632 22.10246981 20.99734632 19.94747901 18.38284993]\n",
      " [19.94747901 20.99734632 19.94747901 18.95010506 18.0025998 ]\n",
      " [18.95010506 19.94747901 18.95010506 18.0025998  17.10246981]\n",
      " [18.0025998  18.95010506 18.0025998  17.10246981 16.24734632]\n",
      " [17.10246981 18.0025998  17.10246981 16.24734632 15.43497901]] \n",
      "\n",
      "[['→' 'o' '←' '←' 'o']\n",
      " ['→' '↑' '←↑' '←↑' '←']\n",
      " ['→' '↑' '←↑' '←↑' '←↑']\n",
      " ['→' '↑' '←↑' '←↑' '←↑']\n",
      " ['→' '↑' '←↑' '←↑' '←↑']] \n",
      "\n",
      "Policy improvement with value iteration:\n",
      "[[20.99734632 22.10246981 20.99734632 19.94747901 18.38284993]\n",
      " [19.94747901 20.99734632 19.94747901 18.95010506 18.0025998 ]\n",
      " [18.95010506 19.94747901 18.95010506 18.0025998  17.10246981]\n",
      " [18.0025998  18.95010506 18.0025998  17.10246981 16.24734632]\n",
      " [17.10246981 18.0025998  17.10246981 16.24734632 15.43497901]] \n",
      "\n",
      "[['→' 'o' '←' '←' 'o']\n",
      " ['→' '↑' '←↑' '←↑' '←']\n",
      " ['→' '↑' '←↑' '←↑' '←↑']\n",
      " ['→' '↑' '←↑' '←↑' '←↑']\n",
      " ['→' '↑' '←↑' '←↑' '←↑']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment, set up parameters\n",
    "the = 1e-10\n",
    "gam = 0.95\n",
    "Env = ENV()\n",
    "\n",
    "# Calculate the value function and find the optimal policy via the three approaches\n",
    "V_opt_1, pol_opt_1 = Bellman_opti_equa(gam, Env)\n",
    "V_opt_2, pol_opt_2 = Policy_iter_with_Iter_policy_eval(the, gam, Env)\n",
    "V_opt_3, pol_opt_3 = Policy_improvement_with_Value_iter(the, gam, Env)\n",
    "\n",
    "# Print the results\n",
    "print('Bellman optimality equation:')\n",
    "print(V_opt_1.reshape([5, -1]), '\\n')\n",
    "print((np.array(print_policy(pol_opt_1, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "print('Policy iteration with iterative policy evaluation:')\n",
    "print(V_opt_2.reshape([5, -1]), '\\n')\n",
    "print((np.array(print_policy(pol_opt_2, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "print('Policy improvement with value iteration:')\n",
    "print(V_opt_3.reshape([5, -1]), '\\n')\n",
    "print((np.array(print_policy(pol_opt_3, Env))).reshape([5, -1]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.997346322099883\n",
      "20.99734632212662\n"
     ]
    }
   ],
   "source": [
    "# due to estimation problem, they are not equal\n",
    "print(V_opt_2[0])\n",
    "print(V_opt_2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellman optimality equation:\n",
      "[[20.997 22.102 20.997 19.947 18.383]\n",
      " [19.947 20.997 19.947 18.95  18.003]\n",
      " [18.95  19.947 18.95  18.003 17.102]\n",
      " [18.003 18.95  18.003 17.102 16.247]\n",
      " [17.102 18.003 17.102 16.247 15.435]] \n",
      "\n",
      "[['→' 'o' '←' '←' 'o']\n",
      " ['→↑' '↑' '←↑' '←↑' '←']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']] \n",
      "\n",
      "Policy iteration with iterative policy evaluation:\n",
      "[[20.997 22.102 20.997 19.947 18.383]\n",
      " [19.947 20.997 19.947 18.95  18.003]\n",
      " [18.95  19.947 18.95  18.003 17.102]\n",
      " [18.003 18.95  18.003 17.102 16.247]\n",
      " [17.102 18.003 17.102 16.247 15.435]] \n",
      "\n",
      "[['→' 'o' '←' '←' 'o']\n",
      " ['→↑' '↑' '←↑' '←↑' '←']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']] \n",
      "\n",
      "Policy improvement with value iteration:\n",
      "[[20.997 22.102 20.997 19.947 18.383]\n",
      " [19.947 20.997 19.947 18.95  18.003]\n",
      " [18.95  19.947 18.95  18.003 17.102]\n",
      " [18.003 18.95  18.003 17.102 16.247]\n",
      " [17.102 18.003 17.102 16.247 15.435]] \n",
      "\n",
      "[['→' 'o' '←' '←' 'o']\n",
      " ['→↑' '↑' '←↑' '←↑' '←']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']\n",
      " ['→↑' '↑' '←↑' '←↑' '←↑']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Therefore, round the value function first, and then find the policy\n",
    "\n",
    "# Round the value function\n",
    "V_opt_1_ = np.round(V_opt_1,3)\n",
    "V_opt_2_ = np.round(V_opt_2,3)\n",
    "V_opt_3_ = np.round(V_opt_3,3)\n",
    "\n",
    "# Find the policy again\n",
    "pol_opt_1_ = find_policy(gam, V_opt_1_, Env)\n",
    "pol_opt_2_ = find_policy(gam, V_opt_2_, Env)\n",
    "pol_opt_3_ = find_policy(gam, V_opt_3_, Env)\n",
    "\n",
    "# Print the results\n",
    "print('Bellman optimality equation:')\n",
    "print(V_opt_1_.reshape([5, -1]), '\\n')\n",
    "print((np.array(print_policy(pol_opt_1_, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "print('Policy iteration with iterative policy evaluation:')\n",
    "print(V_opt_2_.reshape([5, -1]), '\\n')\n",
    "print((np.array(print_policy(pol_opt_2_, Env))).reshape([5, -1]), '\\n')\n",
    "\n",
    "print('Policy improvement with value iteration:')\n",
    "print(V_opt_3_.reshape([5, -1]), '\\n')\n",
    "print((np.array(print_policy(pol_opt_3_, Env))).reshape([5, -1]), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
